{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y-Gws8cuA2-R"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch torchvision pandas scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amis_P38A4Lj",
        "outputId": "474ef2c0-75ba-4ec8-d5f8-341f59e4f4c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, math, random, glob\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "DEFAULT_IMAGES_DIR = \"/content/drive/MyDrive/blue beacon dataset/images1\"\n",
        "DEFAULT_EFFECTS_CSV = \"/content/drive/MyDrive/blue beacon dataset/annotations/Copy of effects.csv - Sheet1 (1).csv\"\n",
        "DEFAULT_CALAM_CSV   = \"/content/drive/MyDrive/blue beacon dataset/annotations/Copy of calamity.csv - Sheet1.csv\"\n",
        "OUT_DIR = \"out_multitask\"\n",
        "IMG_SIZE = 256\n",
        "BATCH = 16\n",
        "EPOCHS = 20\n",
        "LR = 3e-4\n",
        "VAL_SPLIT = 0.10\n",
        "TEST_SPLIT = 0.00\n",
        "SEED = 42\n",
        "ALPHA = 1.0\n",
        "BETA  = 1.0\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def set_seed(s=42):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "\n",
        "\n",
        "def read_csv(p: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(p)\n",
        "    if \"image\" not in df.columns:\n",
        "        raise ValueError(f\"{p} must have an 'image' column\")\n",
        "    df[\"image\"] = df[\"image\"].astype(str).str.strip()\n",
        "\n",
        "    for c in df.columns:\n",
        "        if c == \"image\": continue\n",
        "        df[c] = df[c].map({True:1, False:0}).fillna(df[c]).astype(str).str.strip()\n",
        "        df[c] = df[c].replace({\"\":\"0\",\"nan\":\"0\"}).astype(float)\n",
        "        df[c] = (df[c] >= 0.5).astype(int)\n",
        "    return df\n",
        "\n",
        "def index_all_images(images_dir: str) -> Dict[str, str]:\n",
        "    \"\"\"Return a dict mapping lowercased base filename -> full path.\n",
        "       If you keep subfolders, we also index them; duplicates will pick the first found.\"\"\"\n",
        "    paths = glob.glob(str(Path(images_dir) / \"**/*\"), recursive=True)\n",
        "    mapping = {}\n",
        "    for p in paths:\n",
        "        if os.path.isfile(p):\n",
        "            base = os.path.basename(p).lower()\n",
        "            if base not in mapping:\n",
        "                mapping[base] = p\n",
        "    return mapping\n",
        "\n",
        "def make_merged_df(images_dir: str, effects_csv: str, calam_csv: str) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
        "    df_e = read_csv(effects_csv)\n",
        "    df_c = read_csv(calam_csv)\n",
        "\n",
        "    effect_classes  = [c for c in df_e.columns if c != \"image\"]\n",
        "    calam_classes   = [c for c in df_c.columns if c != \"image\"]\n",
        "    if not effect_classes:  raise ValueError(\"effects.csv needs at least one effect column besides 'image'\")\n",
        "    if not calam_classes:   raise ValueError(\"calamities.csv needs at least one calamity column besides 'image'\")\n",
        "\n",
        "    df = pd.merge(df_e, df_c, on=\"image\", how=\"outer\", suffixes=(\"_e\", \"_c\"))\n",
        "\n",
        "    idx = index_all_images(images_dir)\n",
        "    resolved_paths = []\n",
        "    missing = []\n",
        "    for name in df[\"image\"].tolist():\n",
        "\n",
        "        cand = Path(images_dir) / name\n",
        "        if cand.exists():\n",
        "            resolved_paths.append(str(cand))\n",
        "            continue\n",
        "\n",
        "        key = os.path.basename(name).lower()\n",
        "        if key in idx:\n",
        "            resolved_paths.append(idx[key])\n",
        "        else:\n",
        "            resolved_paths.append(None)\n",
        "            missing.append(name)\n",
        "\n",
        "    if missing:\n",
        "        print(f\"Warning: {len(missing)} images listed in CSVs not found under {images_dir}. They will be skipped.\")\n",
        "        print(\"First few missing:\", missing[:10])\n",
        "\n",
        "    df[\"image_path\"] = resolved_paths\n",
        "    df = df[~df[\"image_path\"].isna()].reset_index(drop=True)\n",
        "    return df, effect_classes, calam_classes\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, effect_cols: List[str], calam_cols: List[str], img_size=256, augment=True):\n",
        "        self.df = df\n",
        "        self.effect_cols = effect_cols\n",
        "        self.calam_cols = calam_cols\n",
        "        if augment:\n",
        "            self.tf = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(10),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "            ])\n",
        "        else:\n",
        "            self.tf = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "            ])\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "        x = self.tf(img)\n",
        "\n",
        "\n",
        "        yE = []\n",
        "        for c in self.effect_cols:\n",
        "            v = row.get(c, np.nan)\n",
        "            yE.append(0 if pd.isna(v) else int(v))\n",
        "        yE = torch.tensor(yE, dtype=torch.float32)\n",
        "        mE = torch.ones_like(yE)\n",
        "\n",
        "\n",
        "        yC = []\n",
        "        for c in self.calam_cols:\n",
        "            v = row.get(c, np.nan)\n",
        "            yC.append(0 if pd.isna(v) else int(v))\n",
        "        yC = torch.tensor(yC, dtype=torch.float32)\n",
        "        mC = torch.ones_like(yC)\n",
        "\n",
        "        return x, yE, mE, yC, mC\n",
        "\n",
        "# Model: one backbone, two heads\n",
        "class MultiTaskEffB0(nn.Module):\n",
        "    def __init__(self, n_effects: int, n_calam: int):\n",
        "        super().__init__()\n",
        "        try:\n",
        "            weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
        "        except:\n",
        "            weights = None\n",
        "        self.backbone = models.efficientnet_b0(weights=weights)\n",
        "        in_feat = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.head_effects = nn.Linear(in_feat, n_effects)\n",
        "        self.head_calam   = nn.Linear(in_feat, n_calam)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone.features(x)\n",
        "        feats = self.pool(feats).flatten(1)\n",
        "        feats = self.dropout(feats)\n",
        "        outE = self.head_effects(feats)\n",
        "        outC = self.head_calam(feats)\n",
        "        return outE, outC\n",
        "\n",
        "\n",
        "def compute_pos_weight(loader, head_idx: int, n_classes: int):\n",
        "    pos = torch.zeros(n_classes)\n",
        "    tot = torch.zeros(n_classes)\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            y = batch[head_idx]\n",
        "            m = batch[head_idx+1]\n",
        "            pos += (y*m).sum(dim=0).cpu()\n",
        "            tot += m.sum(dim=0).cpu()\n",
        "    pos = torch.clamp(pos, min=1.0)\n",
        "    neg = torch.clamp(tot - pos, min=1.0)\n",
        "    return neg / pos\n",
        "\n",
        "\n",
        "def train_multitask(\n",
        "    images_dir=DEFAULT_IMAGES_DIR,\n",
        "    effects_csv=DEFAULT_EFFECTS_CSV,\n",
        "    calam_csv=DEFAULT_CALAM_CSV,\n",
        "    out_dir=OUT_DIR,\n",
        "    img_size=IMG_SIZE,\n",
        "    batch_size=BATCH,\n",
        "    epochs=EPOCHS,\n",
        "    lr=LR,\n",
        "    val_split=VAL_SPLIT,\n",
        "    test_split=TEST_SPLIT,\n",
        "    alpha=ALPHA,\n",
        "    beta=BETA,\n",
        "    seed=SEED\n",
        "):\n",
        "    set_seed(seed)\n",
        "\n",
        "\n",
        "    df_all, effect_cols, calam_cols = make_merged_df(images_dir, effects_csv, calam_csv)\n",
        "    n = len(df_all)\n",
        "    if n == 0:\n",
        "        raise ValueError(\"No images found after matching CSVs to disk. Check names/case/paths.\")\n",
        "\n",
        "\n",
        "    n_test = int(round(n * test_split))\n",
        "    n_val  = int(round(n * val_split))\n",
        "    n_train = n - n_val - n_test\n",
        "    if n_train < 1:\n",
        "        take = 1 - n_train\n",
        "        take_from_val = min(take, n_val); n_val -= take_from_val; take -= take_from_val\n",
        "        take_from_test = min(take, n_test); n_test -= take_from_test\n",
        "        n_train = n - n_val - n_test\n",
        "    print(f\"Split -> train:{n_train} val:{n_val} test:{n_test}\")\n",
        "\n",
        "    dstrain, dsval, dstest = random_split(df_all, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(seed))\n",
        "    ds_tr = MultiTaskDataset(dstrain.dataset.iloc[dstrain.indices], effect_cols, calam_cols, img_size, augment=True)\n",
        "    ds_va = MultiTaskDataset(dsval.dataset.iloc[dsval.indices], effect_cols, calam_cols, img_size, augment=False)\n",
        "    ds_te = MultiTaskDataset(dstest.dataset.iloc[dstest.indices], effect_cols, calam_cols, img_size, augment=False) if n_test>0 else None\n",
        "\n",
        "    tr_loader = DataLoader(ds_tr, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "    va_loader = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    te_loader = DataLoader(ds_te, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True) if ds_te else None\n",
        "\n",
        "    model = MultiTaskEffB0(n_effects=len(effect_cols), n_calam=len(calam_cols)).to(device)\n",
        "\n",
        "\n",
        "    posw_e = compute_pos_weight(tr_loader, head_idx=1, n_classes=len(effect_cols)).to(device)\n",
        "    posw_c = compute_pos_weight(tr_loader, head_idx=3, n_classes=len(calam_cols)).to(device)\n",
        "    crit_e = nn.BCEWithLogitsLoss(pos_weight=posw_e, reduction=\"none\")\n",
        "    crit_c = nn.BCEWithLogitsLoss(pos_weight=posw_c, reduction=\"none\")\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "    best_macro = -1.0\n",
        "    best_path = str(Path(out_dir)/\"best.pt\")\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "\n",
        "        model.train(); run_loss=0.0; seen=0\n",
        "        for x, yE, mE, yC, mC in tqdm(tr_loader, desc=f\"Epoch {ep}/{epochs}\"):\n",
        "            x, yE, mE, yC, mC = x.to(device), yE.to(device), mE.to(device), yC.to(device), mC.to(device)\n",
        "            opt.zero_grad()\n",
        "            oE, oC = model(x)\n",
        "            lossE = (crit_e(oE, yE) * mE).sum() / torch.clamp(mE.sum(), min=1.0)\n",
        "            lossC = (crit_c(oC, yC) * mC).sum() / torch.clamp(mC.sum(), min=1.0)\n",
        "            loss = alpha*lossE + beta*lossC\n",
        "            loss.backward(); opt.step()\n",
        "            run_loss += loss.item() * x.size(0); seen += x.size(0)\n",
        "        sch.step()\n",
        "        train_loss = run_loss / max(seen,1)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            yE_true=[]; yE_pred=[]\n",
        "            yC_true=[]; yC_pred=[]\n",
        "            for x, yE, mE, yC, mC in va_loader:\n",
        "                x = x.to(device)\n",
        "                oE, oC = model(x)\n",
        "                pE = torch.sigmoid(oE).cpu().numpy()\n",
        "                pC = torch.sigmoid(oC).cpu().numpy()\n",
        "                yE_true.append((yE.numpy(), mE.numpy())); yE_pred.append(pE)\n",
        "                yC_true.append((yC.numpy(), mC.numpy())); yC_pred.append(pC)\n",
        "\n",
        "\n",
        "            yE_t = np.concatenate([a for a,_ in yE_true], axis=0)\n",
        "            mE_t = np.concatenate([b for _,b in yE_true], axis=0).astype(bool)\n",
        "            pE_t = (np.concatenate(yE_pred, axis=0) >= 0.5).astype(int)\n",
        "            f1_e = []\n",
        "            for j in range(yE_t.shape[1]):\n",
        "                mj = mE_t[:,j]\n",
        "                if mj.sum()==0: continue\n",
        "                f1_e.append(f1_score(yE_t[mj,j], pE_t[mj,j]))\n",
        "            macro_e = float(np.mean(f1_e)) if f1_e else 0.0\n",
        "\n",
        "\n",
        "            yC_t = np.concatenate([a for a,_ in yC_true], axis=0)\n",
        "            mC_t = np.concatenate([b for _,b in yC_true], axis=0).astype(bool)\n",
        "            pC_t = (np.concatenate(yC_pred, axis=0) >= 0.5).astype(int)\n",
        "            f1_c = []\n",
        "            for j in range(yC_t.shape[1]):\n",
        "                mj = mC_t[:,j]\n",
        "                if mj.sum()==0: continue\n",
        "                f1_c.append(f1_score(yC_t[mj,j], pC_t[mj,j]))\n",
        "            macro_c = float(np.mean(f1_c)) if f1_c else 0.0\n",
        "\n",
        "            macro_avg = (macro_e + macro_c) / 2.0\n",
        "\n",
        "        print(f\"Epoch {ep}: train_loss={train_loss:.4f} | val_macroE={macro_e:.3f} val_macroC={macro_c:.3f} macro_avg={macro_avg:.3f}\")\n",
        "\n",
        "        if macro_avg > best_macro:\n",
        "            best_macro = macro_avg\n",
        "            torch.save({\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"effect_classes\": effect_cols,\n",
        "                \"calamity_classes\": calam_cols,\n",
        "                \"img_size\": img_size\n",
        "            }, best_path)\n",
        "            with open(Path(out_dir)/\"labels.json\",\"w\") as f:\n",
        "                json.dump({\"effects\":effect_cols, \"calamities\":calam_cols}, f, indent=2)\n",
        "            print(\" Saved best to\", best_path)\n",
        "\n",
        "\n",
        "    if te_loader is not None:\n",
        "        print(\"\\n== TEST ==\")\n",
        "        ckpt = torch.load(best_path, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"state_dict\"]); model.eval()\n",
        "        yE_true=[]; yE_pred=[]; yC_true=[]; yC_pred=[]\n",
        "        with torch.no_grad():\n",
        "            for x, yE, mE, yC, mC in te_loader:\n",
        "                x = x.to(device)\n",
        "                oE, oC = model(x)\n",
        "                pE = torch.sigmoid(oE).cpu().numpy()\n",
        "                pC = torch.sigmoid(oC).cpu().numpy()\n",
        "                yE_true.append((yE.numpy(), mE.numpy())); yE_pred.append(pE)\n",
        "                yC_true.append((yC.numpy(), mC.numpy())); yC_pred.append(pC)\n",
        "\n",
        "\n",
        "        yE_t = np.concatenate([a for a,_ in yE_true], axis=0)\n",
        "        mE_t = np.concatenate([b for _,b in yE_true], axis=0).astype(bool)\n",
        "        pE_t = (np.concatenate(yE_pred, axis=0) >= 0.5).astype(int)\n",
        "        f1_e = [f1_score(yE_t[mE_t[:,j],j], pE_t[mE_t[:,j],j]) for j in range(yE_t.shape[1]) if mE_t[:,j].sum()>0]\n",
        "        print(\"Effects per-class F1:\", [round(x,3) for x in f1_e], \"Macro:\", round(float(np.mean(f1_e)) if f1_e else 0.0,3))\n",
        "\n",
        "        yC_t = np.concatenate([a for a,_ in yC_true], axis=0)\n",
        "        mC_t = np.concatenate([b for _,b in yC_true], axis=0).astype(bool)\n",
        "        pC_t = (np.concatenate(yC_pred, axis=0) >= 0.5).astype(int)\n",
        "        f1_c = [f1_score(yC_t[mC_t[:,j],j], pC_t[mC_t[:,j],j]) for j in range(yC_t.shape[1]) if mC_t[:,j].sum()>0]\n",
        "        print(\"Calamities per-class F1:\", [round(x,3) for x in f1_c], \"Macro:\", round(float(np.mean(f1_c)) if f1_c else 0.0,3))\n",
        "\n",
        "    print(\"\\nDone. Artifacts in:\", out_dir)\n",
        "    return best_path\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_image(img_path: str, ckpt_path: str):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    eff = ckpt[\"effect_classes\"]; cal = ckpt[\"calamity_classes\"]; size = ckpt[\"img_size\"]\n",
        "    model = MultiTaskEffB0(n_effects=len(eff), n_calam=len(cal)).to(device)\n",
        "    model.load_state_dict(ckpt[\"state_dict\"]); model.eval()\n",
        "\n",
        "    tf = transforms.Compose([\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "    ])\n",
        "    im = Image.open(img_path).convert(\"RGB\")\n",
        "    x = tf(im).unsqueeze(0).to(device)\n",
        "    oE, oC = model(x)\n",
        "    pE = torch.sigmoid(oE).squeeze(0).cpu().numpy()\n",
        "    pC = torch.sigmoid(oC).squeeze(0).cpu().numpy()\n",
        "    return dict(effects=list(zip(eff, pE.tolist())), calamities=list(zip(cal, pC.tolist())))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_7KND5jBID1",
        "outputId": "4a0d1151-38b5-4038-f70a-9ef1182daa0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_ckpt = train_multitask(\n",
        "    images_dir=\"/content/drive/MyDrive/blue beacon dataset/images1\",\n",
        "    effects_csv=\"/content/drive/MyDrive/blue beacon dataset/annotations/Copy of effects.csv - Sheet1 (1).csv\",\n",
        "    calam_csv=\"/content/drive/MyDrive/blue beacon dataset/annotations/Copy of calamity.csv - Sheet1.csv\",\n",
        "    out_dir=\"out_multitask\",\n",
        "    img_size=256,\n",
        "    batch_size=16,\n",
        "    epochs=20,\n",
        "    lr=3e-4,\n",
        "    val_split=0.10,\n",
        "    test_split=0.00,\n",
        "    alpha=1.0,\n",
        "    beta=1.0,\n",
        "    seed=42\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETZ4fzxKB_7g",
        "outputId": "1ccbe860-fbb3-492f-a63a-8584a0406400"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split -> train:13 val:2 test:0\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 215MB/s]\n",
            "Epoch 1/20: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss=1.8239 | val_macroE=0.222 val_macroC=0.333 macro_avg=0.278\n",
            "✅ Saved best to out_multitask/best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_loss=1.6962 | val_macroE=0.222 val_macroC=0.333 macro_avg=0.278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss=1.5634 | val_macroE=0.444 val_macroC=0.333 macro_avg=0.389\n",
            "✅ Saved best to out_multitask/best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train_loss=1.4029 | val_macroE=0.556 val_macroC=0.667 macro_avg=0.611\n",
            "✅ Saved best to out_multitask/best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: train_loss=1.3287 | val_macroE=0.556 val_macroC=0.667 macro_avg=0.611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: train_loss=1.1892 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: train_loss=1.1103 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: train_loss=1.0825 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: train_loss=0.9442 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: train_loss=0.8758 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: train_loss=0.8722 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: train_loss=0.8444 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: train_loss=0.7834 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: train_loss=0.7358 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: train_loss=0.7316 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: train_loss=0.6928 | val_macroE=0.556 val_macroC=0.556 macro_avg=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: train_loss=0.6681 | val_macroE=0.667 val_macroC=0.556 macro_avg=0.611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: train_loss=0.6410 | val_macroE=0.667 val_macroC=0.556 macro_avg=0.611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: train_loss=0.6596 | val_macroE=0.667 val_macroC=0.556 macro_avg=0.611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: train_loss=0.6701 | val_macroE=0.667 val_macroC=0.667 macro_avg=0.667\n",
            "✅ Saved best to out_multitask/best.pt\n",
            "\n",
            "Done. Artifacts in: out_multitask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7gIyl6qoCUFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch, json\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "CKPT_PATH = \"/content/out_multitask/best.pt\"\n",
        "IMG_PATH  = \"/content/drive/MyDrive/blue beacon dataset/images1/oilspill_1.jpg\"\n",
        "\n",
        "\n",
        "class MultiTaskEffB0(nn.Module):\n",
        "    def __init__(self, n_effects: int, n_calam: int):\n",
        "        super().__init__()\n",
        "        try:\n",
        "            weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
        "        except:\n",
        "            weights = None\n",
        "        self.backbone = models.efficientnet_b0(weights=weights)\n",
        "        in_feat = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.head_effects = nn.Linear(in_feat, n_effects)\n",
        "        self.head_calam   = nn.Linear(in_feat, n_calam)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone.features(x)\n",
        "        feats = self.pool(feats).flatten(1)\n",
        "        feats = self.dropout(feats)\n",
        "        return self.head_effects(feats), self.head_calam(feats)\n",
        "\n",
        "\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "eff_labels = ckpt[\"effect_classes\"]\n",
        "cal_labels = ckpt[\"calamity_classes\"]\n",
        "img_size   = ckpt.get(\"img_size\", 256)\n",
        "\n",
        "model = MultiTaskEffB0(n_effects=len(eff_labels), n_calam=len(cal_labels)).to(device)\n",
        "model.load_state_dict(ckpt[\"state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "\n",
        "tf = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "im = Image.open(IMG_PATH).convert(\"RGB\")\n",
        "x = tf(im).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outE, outC = model(x)\n",
        "    pE = torch.sigmoid(outE).squeeze(0).cpu().numpy()\n",
        "    pC = torch.sigmoid(outC).squeeze(0).cpu().numpy()\n",
        "\n",
        "\n",
        "def topk(names, probs, k=5):\n",
        "    order = sorted(range(len(names)), key=lambda i: probs[i], reverse=True)[:k]\n",
        "    return [(names[i], float(probs[i])) for i in order]\n",
        "\n",
        "print(\"\\nTop Effects:\")\n",
        "for name, p in topk(eff_labels, pE, k=min(5, len(eff_labels))):\n",
        "    print(f\"  {name:24s}  {p:.3f}\")\n",
        "\n",
        "print(\"\\nTop Calamities:\")\n",
        "for name, p in topk(cal_labels, pC, k=min(5, len(cal_labels))):\n",
        "    print(f\"  {name:24s}  {p:.3f}\")\n",
        "\n",
        "\n",
        "thresholds = {lbl: 0.50 for lbl in cal_labels}\n",
        "for k in thresholds:\n",
        "    if k.lower() in [\"tsunami\"]: thresholds[k] = 0.80\n",
        "    if k.lower() in [\"oil_spill\",\"ship_boat_wreckage\"]: thresholds[k] = 0.70\n",
        "\n",
        "accepted_calams = [(cal_labels[i], float(pC[i])) for i in range(len(cal_labels)) if pC[i] >= thresholds[cal_labels[i]]]\n",
        "abstain = (len(accepted_calams) == 0)\n",
        "\n",
        "result = {\n",
        "    \"effects\": [{\"label\": eff_labels[i], \"p\": float(pE[i])} for i in range(len(eff_labels))],\n",
        "    \"calamities\": [{\"label\": cal_labels[i], \"p\": float(pC[i])} for i in range(len(cal_labels))],\n",
        "    \"accepted_calamities\": sorted(accepted_calams, key=lambda x: x[1], reverse=True),\n",
        "    \"abstain\": abstain\n",
        "}\n",
        "\n",
        "print(\"\\nAccepted calamities (thresholded):\", result[\"accepted_calamities\"])\n",
        "print(\"Abstain on cause?:\", result[\"abstain\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A2OwambFpBd",
        "outputId": "2e0d8a8b-7afc-4548-d9f0-ea6bf801870f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "Top Effects:\n",
            "  oil_sheen                 0.860\n",
            "  debris_on_beach           0.353\n",
            "  flooded_area              0.292\n",
            "\n",
            "Top Calamities:\n",
            "  oil_spill                 0.847\n",
            "  debris                    0.306\n",
            "  floods                    0.198\n",
            "\n",
            "Accepted calamities (thresholded): [('oil_spill', 0.8465505242347717)]\n",
            "Abstain on cause?: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39TMKIfYF3aP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}